{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  id\n",
      "0  [16, 231, 543, 5, 15, 43, 8282, 94, 231, 1129,...      1   0\n",
      "1  [16, 4046, 138, 10, 2, 1809, 2007, 3763, 14, 4...      1   1\n",
      "2  [1108, 16550, 3, 6168, 3, 160, 284, 19, 49, 46...      1   2\n",
      "3  [1802, 27, 16, 25, 48, 451, 632, 3, 2, 2164, 2...      1   3\n",
      "4  [16, 19, 302, 93, 97, 43, 952, 118, 1, 16, 528...      1   4\n",
      "                                                text  label    id\n",
      "0  [12, 920, 7, 1266, 28, 9884, 1640, 116, 11, 13...      1  5000\n",
      "1  [783, 397, 253, 5797, 9379, 22, 793, 11838, 10...      1  5001\n",
      "2  [888, 14851, 323, 9, 27, 1377, 584, 195, 3, 13...      1  5002\n",
      "3  [228, 1161, 5815, 379, 9, 941, 10, 2, 316, 4, ...      1  5003\n",
      "4  [736, 19, 37, 813, 45, 6723, 27, 626, 8, 2, 34...      1  5004\n",
      "Training set size: 10578\n",
      "Test set size: 11\n"
     ]
    }
   ],
   "source": [
    "#  Load the data\n",
    "domain1_train_data = pd.read_json('domain1_train_data.json', lines=True)\n",
    "domain2_train_data = pd.read_json('domain2_train_data.json', lines=True)\n",
    "\n",
    "\n",
    "print(domain1_train_data.head())\n",
    "print(domain2_train_data.head())\n",
    "\n",
    "# get machine and human data\n",
    "machine = domain2_train_data[domain2_train_data['label'] == 0]\n",
    "human = domain2_train_data[domain2_train_data['label'] == 1]\n",
    "\n",
    "# count the number of samples in each class\n",
    "n_machine = len(machine)\n",
    "n_human = len(human)\n",
    "\n",
    "# Filter machine data for samples with token length less than 1000\n",
    "machine_filtered = machine[machine['text'].apply(len) < 2000]\n",
    "\n",
    "# Count the number of samples again after filtering\n",
    "n_machine_filtered = len(machine_filtered)\n",
    "\n",
    "for label in [0, 1]:  # 0 代表 machine, 1 代表 human\n",
    "    class_samples = domain1_train_data[domain1_train_data['label'] == label]\n",
    "    n_samples = len(class_samples)\n",
    "    if n_samples < 2600:\n",
    "        extra_samples_needed = 2600 - n_samples\n",
    "        sampled_data = class_samples.sample(n=extra_samples_needed, replace=True, random_state=42)\n",
    "        domain1_train_data = pd.concat([domain1_train_data, sampled_data])\n",
    "\n",
    "# Adjust machine_balanced to have exactly 2500 samples if possible\n",
    "if n_machine_filtered > 2600:\n",
    "    machine_balanced = machine_filtered.sample(n=2600, random_state=42)\n",
    "else:\n",
    "    machine_balanced = machine_filtered\n",
    "    print(\"Note: Machine data has less than 2500 samples after filtering for token length.\")\n",
    "\n",
    "# Oversample human data to reach 2500 samples\n",
    "if n_human < 2600:\n",
    "    extra_samples_needed = 2600 - n_human\n",
    "    human_balanced = pd.concat([human, human.sample(n=extra_samples_needed, replace=True, random_state=42)])\n",
    "else:\n",
    "    human_balanced = human\n",
    "\n",
    "# Combine the balanced data\n",
    "domain2_train_data_balanced = pd.concat([machine_balanced, human_balanced])\n",
    "\n",
    "# domain2 label count\n",
    "domain2_train_data_balanced['label'].value_counts()\n",
    "\n",
    "# combine the two datasets\n",
    "combined_data = pd.concat([domain1_train_data, domain2_train_data_balanced])\n",
    "\n",
    "\n",
    "# get the features and labels\n",
    "X = combined_data['text']\n",
    "y = combined_data['label']\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.001, random_state=42)\n",
    "\n",
    "# split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.001, random_state=42)\n",
    "\n",
    "X_train_str = [' '.join(map(str, lst)) for lst in X_train]\n",
    "X_val_str = [' '.join(map(str, lst)) for lst in X_val]\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label    id\n",
      "0     [16, 231, 543, 5, 15, 43, 8282, 94, 231, 1129,...      1     0\n",
      "1     [16, 4046, 138, 10, 2, 1809, 2007, 3763, 14, 4...      1     1\n",
      "2     [1108, 16550, 3, 6168, 3, 160, 284, 19, 49, 46...      1     2\n",
      "3     [1802, 27, 16, 25, 48, 451, 632, 3, 2, 2164, 2...      1     3\n",
      "4     [16, 19, 302, 93, 97, 43, 952, 118, 1, 16, 528...      1     4\n",
      "...                                                 ...    ...   ...\n",
      "161   [1800, 7841, 8, 7, 4159, 4480, 2361, 87, 186, ...      1  5161\n",
      "1403  [1291, 117, 100, 26, 344, 5, 600, 7925, 620, 6...      1  6403\n",
      "86    [181, 117, 100, 14, 6369, 13, 22, 472, 8, 110,...      1  5086\n",
      "54    [2, 136, 401, 6, 496, 1136, 8, 486, 117, 100, ...      1  5054\n",
      "1499  [8, 15, 71, 12, 155, 27, 9189, 117, 67, 10, 10...      1  6499\n",
      "\n",
      "[5400 rows x 3 columns]\n",
      "                                                   text  label     id\n",
      "2500  [20112, 31, 31, 31, 31, 31, 31, 497, 8, 15, 27...      0   2500\n",
      "2501  [16, 395, 3347, 3764, 6, 351, 7662, 3, 33, 124...      0   2501\n",
      "2502  [107, 8823, 1221, 405, 3491, 4624, 1585, 28, 1...      0   2502\n",
      "2503  [16, 395, 15, 128, 31, 2, 2193, 9, 27, 1252, 2...      0   2503\n",
      "2504  [5211, 2, 567, 10, 33, 10328, 28, 3281, 5, 334...      0   2504\n",
      "...                                                 ...    ...    ...\n",
      "4992  [15, 71, 4817, 2, 57, 4, 306, 1023, 10, 5016, ...      0   9992\n",
      "3106  [2178, 61, 1672, 145, 1223, 5, 494, 2, 9599, 6...      0   8106\n",
      "9137  [225, 1975, 22, 1287, 1476, 18, 1903, 4983, 11...      0  14137\n",
      "4120  [7826, 754, 2, 5281, 175, 183, 10, 4339, 2774,...      0   9120\n",
      "6398  [110, 116, 10, 2635, 194, 3, 1880, 3, 67, 1375...      0  11398\n",
      "\n",
      "[5200 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# count domain1_train_data human and machine\n",
    "human = combined_data[combined_data['label'] == 1]\n",
    "machine = combined_data[combined_data['label'] == 0]\n",
    "\n",
    "print(human)\n",
    "print(machine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label    id\n",
      "0     [12, 920, 7, 1266, 28, 9884, 1640, 116, 11, 13...      1  5000\n",
      "1     [783, 397, 253, 5797, 9379, 22, 793, 11838, 10...      1  5001\n",
      "2     [888, 14851, 323, 9, 27, 1377, 584, 195, 3, 13...      1  5002\n",
      "3     [228, 1161, 5815, 379, 9, 941, 10, 2, 316, 4, ...      1  5003\n",
      "4     [736, 19, 37, 813, 45, 6723, 27, 626, 8, 2, 34...      1  5004\n",
      "...                                                 ...    ...   ...\n",
      "161   [1800, 7841, 8, 7, 4159, 4480, 2361, 87, 186, ...      1  5161\n",
      "1403  [1291, 117, 100, 26, 344, 5, 600, 7925, 620, 6...      1  6403\n",
      "86    [181, 117, 100, 14, 6369, 13, 22, 472, 8, 110,...      1  5086\n",
      "54    [2, 136, 401, 6, 496, 1136, 8, 486, 117, 100, ...      1  5054\n",
      "1499  [8, 15, 71, 12, 155, 27, 9189, 117, 67, 10, 10...      1  6499\n",
      "\n",
      "[2800 rows x 3 columns]\n",
      "                                                   text  label     id\n",
      "7056  [2295, 96, 237, 2557, 3, 57, 518, 3, 2097, 307...      0  12056\n",
      "3070  [2, 991, 9, 5, 24, 8, 8820, 8661, 459, 3, 36, ...      0   8070\n",
      "4082  [214, 432, 22, 10947, 28, 110, 265, 87, 5, 754...      0   9082\n",
      "6448  [15, 71, 1311, 2, 10871, 1672, 145, 68, 3, 348...      0  11448\n",
      "8589  [12, 148, 1598, 1622, 78, 5, 625, 2, 3276, 141...      0  13589\n",
      "...                                                 ...    ...    ...\n",
      "4992  [15, 71, 4817, 2, 57, 4, 306, 1023, 10, 5016, ...      0   9992\n",
      "3106  [2178, 61, 1672, 145, 1223, 5, 494, 2, 9599, 6...      0   8106\n",
      "9137  [225, 1975, 22, 1287, 1476, 18, 1903, 4983, 11...      0  14137\n",
      "4120  [7826, 754, 2, 5281, 175, 183, 10, 4339, 2774,...      0   9120\n",
      "6398  [110, 116, 10, 2635, 194, 3, 1880, 3, 67, 1375...      0  11398\n",
      "\n",
      "[2600 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# count domain2_train_data_balanced human and machine\n",
    "human = domain2_train_data_balanced[domain2_train_data_balanced['label'] == 1]\n",
    "machine = domain2_train_data_balanced[domain2_train_data_balanced['label'] == 0]\n",
    "\n",
    "print(human)\n",
    "print(machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf向量化\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "# vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train_str)\n",
    "X_val_vec = vectorizer.transform(X_val_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Logistic Regression Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      1.00      0.50         1\n",
      "           1       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.82        11\n",
      "   macro avg       0.67      0.90      0.69        11\n",
      "weighted avg       0.94      0.82      0.85        11\n",
      "\n",
      "Training Support Vector Machine...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize models\n",
    "log_reg_model = LogisticRegression(max_iter=1000)\n",
    "# kernel = rbf\n",
    "svm_model = SVC(kernel=\"rbf\")\n",
    "rf_model = RandomForestClassifier(n_estimators=350)\n",
    "nn_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": log_reg_model,\n",
    "    \"Support Vector Machine\": svm_model,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"Neural Network\": nn_model\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    y_val_pred = model.predict(X_val_vec)\n",
    "    print(f\"{name} Performance:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 读取测试数据\n",
    "# test_data = pd.read_json('test_data.json', lines=True)\n",
    "\n",
    "# # json文件是一个字典，其中包含一个键为'text'的项，其值是一个包含所有测试样本的列表\n",
    "# test_texts = [' '.join(map(str, lst)) for lst in test_data['text']]\n",
    "\n",
    "# # 使用相同的向量化器来转换测试数据\n",
    "# X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "# # 使用模型进行预测\n",
    "# predictions = rf_model.predict(X_test)\n",
    "\n",
    "# # 创建一个数据框，其中'id'列是样本的ID（通常是一个从0开始的整数序列），'class'列是预测结果\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': range(len(predictions)),\n",
    "#     'class': predictions\n",
    "# })\n",
    "\n",
    "# submission.to_csv('new_results/RF24_3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SML_A1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
