# Machine-Generated Text Detection

## Overview

This repository demonstrates solutions for the problem of determining whether a piece of text is authored by a human or generated by a machine. The task involves training models using labelled examples (text, label) and producing predictions for unseen texts. Each instance in the training set consists of a sequence of token indices, a binary label (0 for machine generated, 1 for human generated) and an identifier【890939888370210†L20-L40】. The training data come from two distinct domains representing different topics; Domain 1 is balanced, while Domain 2 is heavily imbalanced with far fewer human‑written samples【890939888370210†L51-L63】【890939888370210†L75-L92】. The goal is to build models that generalise across domains and handle class imbalance【890939888370210†L51-L63】.

## Files and Notebooks

- `baseline.ipynb`: TF‑IDF + logistic regression baseline.
- `data_augmentation.ipynb`: Rebalances classes by oversampling minority samples and concatenating Domain 1 and Domain 2 training data【614772688487699†L55-L117】.
- `svm_diff_domain.ipynb`: Trains separate SVM models for each domain.
- `adaboost.ipynb`: AdaBoost and AdaBoost + SVM stacking.
- `mlp_model.ipynb`: Multi‑layer perceptron in PyTorch using TF‑IDF features【57984846770243†L9-L14】.
- `lstm_model.ipynb`: LSTM using sequence embeddings.
- `meta_model.ipynb`: Stacking models trained on each domain and on the combined data【57984846770243†L12-L14】.
- `random_forest.ipynb`: Placeholder for a tree‑based model.

## Dataset

Two newline‑delimited JSON files provide the training data. `domain1.json` contains 5,000 samples (2,500 per class) and `domain2.json` contains 13,000 samples (1,500 human‑generated and 11,500 machine‑generated)【890939888370210†L75-L92】. Each line is a dictionary with fields:
- `text`: a list of token indices representing the pre‑processed words【890939888370210†L84-L86】;
- `label`: a binary label (0 = machine, 1 = human)【890939888370210†L87-L88】;
- `id`: an identifier.

A separate test set comprises 4,000 samples, split evenly between domains and classes【890939888370210†L95-L99】. To run the notebooks locally, place `domain1.json` and `domain2.json` in the project root and adjust file paths as needed. During data augmentation the minority class is oversampled and the domains are concatenated into one training set【614772688487699†L55-L117】.

## Requirements

See `requirements.txt` for a list of dependencies (pandas, numpy, scikit‑learn, gensim, matplotlib, seaborn, torch, etc.). Create a virtual environment and run `pip install -r requirements.txt`.

## Usage

Open a notebook in Jupyter and run the cells sequentially. Adjust the file paths for the JSON data as needed. Experiment by tuning hyper‑parameters or adding new models. Contributions are welcome!

## Future Work

Ideas for further improvement include implementing a proper random forest model in `random_forest.ipynb`, exploring transformer‑based architectures, and investigating more sophisticated domain adaptation and data augmentation techniques.
